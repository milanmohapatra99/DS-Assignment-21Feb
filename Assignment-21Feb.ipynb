{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5be15b-23eb-47aa-9095-b451bf7dcbca",
   "metadata": {},
   "source": [
    "Que:1. What is Web Scraping?Why isit Used?Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d8ea28-08eb-4843-9072-297c47f82b1c",
   "metadata": {},
   "source": [
    "Ans : Web scraping refers to the extraction of data from a website. This information is collected and then exported into a format that is more useful for the user.\n",
    "\n",
    "Web Scraping used:\n",
    "\n",
    "Data scraping has numerous applications across many industries—including insurance, banking, finance, trading, eCommerce, sports, and digital marketing.\n",
    "Data is also used to inform decision-making, generate leads and sales, manage risks, guide strategies, and create new products and services.    \n",
    "    \n",
    "Web Scraping has multiple applications across various industries.  They are:\n",
    "    \n",
    "1. Price Monitoring :\n",
    "    \n",
    "Price Monitoring refers to monitoring a competitor’s prices and responding to their changes in pricing. Retailers use price Monitoring to maintain a competitive edge over their rivals.\n",
    "\n",
    "Effective price Monitoring involves web scraping, with eCommerce sellers extracting product and pricing information from other eCommerce websites to guide their pricing and marketing decisions.\n",
    "\n",
    "Price Monitoring remains one of the most prominent use cases for web scraping due to valuable data for revenue optimization, product trend monitoring, dynamic pricing, competitor monitoring, and other applications. \n",
    "\n",
    "\n",
    "2. Market Research:\n",
    "    \n",
    "Web data extraction plays a vital role in market research. Market researchers use the resulting data to inform their market trend analysis, research and development, competitor analysis, price analysis, and other areas of study.\n",
    " \n",
    "\n",
    "3. Real Estate :\n",
    "\n",
    "You need web data extraction to generate the most up-to-date and accurate real estate listings. Web scraping is commonly used to retrieve the most updated data about properties, sale prices, monthly rental income, amenities, property agents, and other data points.\n",
    "\n",
    "Web scraped data also informs property value appraisals, rental yield estimates, and real estate market trends analysis. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe423087-10f3-4e8b-88f2-9428b19bce72",
   "metadata": {},
   "source": [
    "Que:2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22225950-943f-426c-a9c6-538be4266a15",
   "metadata": {},
   "source": [
    "Ans: There are various methods and tools used for web scraping, depending on the complexity of the task and the type of data being collected.\n",
    "Here are some of the most common methods:\n",
    "    \n",
    "1.Google Sheets:\n",
    "Google Sheets is a popular tool for data scraping. Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website. \n",
    "This command also makes it possible to check if a website can be scraped or is protected.\n",
    "\n",
    "2. Manual Scraping :\n",
    "This methods having ability to copy/paste information and a spreadsheet to keep track of the extracted data.\n",
    "\n",
    "3. Automated Scraping:\n",
    "Automated web scraping involves easy to use and savings in time and costs.Automated web scraping can be more efficient than manual scraping, but it requires some programming knowledge.\n",
    "\n",
    "4. DOM Parsing: \n",
    "In order to dynamically modify or inspect a web page, client-side scripts parse the contents of the web page into a DOM tree. By embedding a program into the web browser, you can then retrieve the information from the tree.\n",
    "\n",
    "5. Text Grepping:\n",
    "Using Python programming languages or Perl, one can use the UNIX grep command to extract valuable data and information from web pages.\n",
    "\n",
    "6. HTTP Programming:\n",
    "Using socket programming, posting HTTP requests can help one retrieve dynamic as well as static web page information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f556a7-9dfb-47d3-989b-f4d423fe64cf",
   "metadata": {},
   "source": [
    "Que:3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b9e7d-8604-4c2b-86a6-9576c469f0d0",
   "metadata": {},
   "source": [
    "Ans: Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.\n",
    "\n",
    "Beautiful Soup is a great tool for extracting very specific information from large unstructured raw Data, and also it is very fast and handy to use.\n",
    "\n",
    "Features of Beautiful Soup: \n",
    "Some key features that make beautiful soup unique are:\n",
    "\n",
    "Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\n",
    "\n",
    "Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "\n",
    "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, which allows​ us to try out different parsing strategies or trade speed for flexibility.\n",
    "\n",
    "Beautiful Soup allows you to search for specific tags, attributes, and text content within a document, making it easy to extract the data you need.\n",
    "\n",
    "Beautiful Soup provides a simple and intuitive interface for parsing HTML and XML documents. It can handle nested tags, malformed HTML, and other common issues that can arise when scraping web pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d1068-d432-4473-b7e0-6cc1720cdb68",
   "metadata": {},
   "source": [
    "Que:4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f43bb-7847-4419-9e62-eddc85d54051",
   "metadata": {},
   "source": [
    "Ans: Flask is a microframework for developers, designed to enable them to create and scale web apps quickly and simply.\n",
    "\n",
    "Flask is a lightweight framework to build websites. We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "\n",
    "Flask allows users to enter search parameters or URLs, and then display the scraped data in a user-friendly format. Flask can also handle routing, templating, and other web-related tasks, making it easy to build a functional and responsive web interface for your scraping project.\n",
    "\n",
    "Here are some specific reasons why Flask might be used in a web scraping project:\n",
    "higher flexibility.\n",
    "higher compatibility with latest technologies.\n",
    "high scalability for simple web applications.\n",
    "technical experimentation.\n",
    "customization.\n",
    "slightly higher framework performance.\n",
    "easier to use for simple cases.\n",
    "smaller size of the code base.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962f1a5-1edf-419e-9c30-dcf3f1ff6659",
   "metadata": {},
   "source": [
    "Que:5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df527e-1605-45e6-bca8-32b236d499d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: We uses CodePipeline and Elastic Beanstalk in this project.\n",
    "\n",
    "CodePipeline:\n",
    "CodePipeline  automates the software deployment process, allowing a developer to quickly model, visualize and deliver code for new features and updates. \n",
    "\n",
    "AWS CodePipeline automatically builds, tests and launches an application each time the code is changed; a developer uses a graphic user interface to model workflow configurations for the release process within the pipeline.\n",
    "\n",
    "We can integrate CodePipeline with other AWS services, such as AWS CodeBuild and AWS CodeDeploy, to automate our entire software release process. We can also use CodePipeline with third-party tools and services, such as Jenkins and GitHub, to customize our pipeline and incorporate our existing workflows.\n",
    "\n",
    "By using CodePipeline, we can increase the speed and reliability of our software release process, reduce manual errors, and improve collaboration between development and operations teams.\n",
    "\n",
    "\n",
    "Elastic Beanstalk:\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring.\n",
    "\n",
    "Elastic beanstalk is a pre-configured EC2 server that can directly take up your application code and environment configurations and use it to automatically provision and deploy the required resources within AWS to run the web application. \n",
    "\n",
    "Elastic Beanstalk offers us a range of deployment options, including rolling updates, blue/green deployments, and canary deployments. This enables us to choose the deployment method that best suits our application and business needs.\n",
    "\n",
    "Overall, Elastic Beanstalk simplifies the process of deploying and managing web applications and services on AWS, allowing us to focus on writing code and building our business.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b037809-8843-48ed-bee8-80fa27f45da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2da79-34b1-42fc-b90c-b8df150d065f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
